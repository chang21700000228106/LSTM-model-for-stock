{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNBC AND YAHOO CRAWLING (BY API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\user\\anaconda3\\lib\\site-packages (4.27.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.28.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: newsapi-python in c:\\users\\user\\anaconda3\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: requests<3.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from newsapi-python) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0->newsapi-python) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0->newsapi-python) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0->newsapi-python) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3.0.0->newsapi-python) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匯入所需的套件\n",
    "import pandas as pd  # 用於資料處理和儲存\n",
    "from bs4 import BeautifulSoup  # 用於解析 HTML 網頁內容\n",
    "from datetime import datetime  # 用於取得當前日期和時間\n",
    "from selenium import webdriver  # 用於自動化瀏覽器操作\n",
    "from selenium.webdriver.common.by import By  # 用於定位網頁元素\n",
    "from selenium.webdriver.chrome.options import Options  # 用於設定 Chrome 瀏覽器選項\n",
    "from selenium.webdriver.support.ui import WebDriverWait  # 用於等待網頁元素載入\n",
    "from selenium.webdriver.support import expected_conditions as EC  # 用於設定等待條件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')  # 啟用無頭模式，不顯示瀏覽器視窗\n",
    "    options.add_argument('window-size=800x600')  # 設定瀏覽器視窗大小\n",
    "    prefs = {\"profile.managed_default_content_settings.images\": 2}  # 禁止載入圖片，加快爬取速度\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    driver = webdriver.Chrome(options=options)  # 初始化 Chrome 瀏覽器\n",
    "    return driver\n",
    "\n",
    "# 初始化一個空的列表來儲存新聞標題\n",
    "title_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Yahoo():\n",
    "    # -------------------- Yahoo Finance 新聞抓取 --------------------\n",
    "\n",
    "    # 設置 Selenium 瀏覽器並打開 Yahoo Finance 的 Tesla 新聞頁面\n",
    "    driver = setup_driver()\n",
    "    yahoo_url = \"https://finance.yahoo.com/quote/TSLA/news\"\n",
    "    driver.get(yahoo_url)  # 載入頁面\n",
    "\n",
    "    # 取得頁面源代碼並使用 BeautifulSoup 解析\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # 尋找包含新聞的主要 div 區塊\n",
    "    news_div = soup.find(\"ul\", class_=\"stream-items yf-1usaaz9\").find_all(\"li\", class_=\"stream-item story-item yf-1usaaz9\")\n",
    "\n",
    "    # 迭代每個新聞項目，提取標題\n",
    "    for news_item in news_div:\n",
    "        try:\n",
    "            # 嘗試找到標題的 h3 標籤並提取文字\n",
    "            title = news_item.find(\"h3\", class_=\"clamp yf-18q3fnf\").text\n",
    "            title_list.append(title.strip())  # 去除前後空白並加入列表\n",
    "        except AttributeError:\n",
    "            print(\"err finding\" + news_item.text)\n",
    "            # 如果找不到標題，跳過此新聞項目\n",
    "            continue\n",
    "\n",
    "    # 關閉 Yahoo Finance 的瀏覽器實例\n",
    "    driver.quit()\n",
    "#Yahoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNBC():\n",
    "    # -------------------- CNBC 新聞抓取 --------------------\n",
    "\n",
    "    # 設置 Selenium 瀏覽器並打開 CNBC 的 Tesla 搜尋結果頁面\n",
    "    driver = setup_driver()\n",
    "    cnbc_url = \"https://www.cnbc.com/search/?query=tesla&qsearchterm=tesla\"\n",
    "    driver.get(cnbc_url)  # 載入頁面\n",
    "\n",
    "    # 使用 WebDriverWait 等待搜尋結果容器載入完成\n",
    "    wait = WebDriverWait(driver, 60)  # 最多等待 60 秒\n",
    "    search_results = wait.until(EC.visibility_of_element_located((By.CLASS_NAME, \"SearchResults-searchResultsContainer\")))\n",
    "\n",
    "    # 取得頁面源代碼並使用 BeautifulSoup 解析\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # 尋找包含搜尋結果的主要 div 區塊\n",
    "    news_div_cnbc = soup.find(\"div\", class_=\"SearchResults-searchResultsContainer\").find(\"div\", id=\"searchcontainer\").find_all(\"div\")\n",
    "\n",
    "    # 迭代每個新聞項目，提取標題\n",
    "    for news_item in news_div_cnbc:\n",
    "        try:\n",
    "            # 嘗試找到標題的 span 標籤並提取文字\n",
    "            title = news_item.find(\"div\", class_=\"SearchResult-searchResultTitle\").find(\"a\").find(\"span\").text\n",
    "            title_list.append(title.strip())  # 去除前後空白並加入列表\n",
    "        except AttributeError:\n",
    "            # 如果找不到標題，跳過此新聞項目\n",
    "            continue\n",
    "\n",
    "    # 關閉 CNBC 的瀏覽器實例\n",
    "    driver.quit()\n",
    "#CNBC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from newsapi.newsapi_client import NewsApiClient\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "newsapi = NewsApiClient(api_key='e862efc88eeb445a9f54d97d9174ac69')\n",
    "title_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveTitles():\n",
    "    global dates, csv_filename\n",
    "    # -------------------- 去除重複標題並儲存到 CSV --------------------\n",
    "\n",
    "    # 去除列表中的重複標題\n",
    "    i = 0\n",
    "    while i < len(title_list):\n",
    "        if title_list.count(title_list[i]) > 1:\n",
    "            title_list.pop(i)\n",
    "            dates.pop(i)\n",
    "        else:\n",
    "            i+=1\n",
    "    # unique_titles = list(dict.fromkeys(title_list))\n",
    "\n",
    "    # 創建一個 Pandas DataFrame，包含當前日期和新聞標題\n",
    "    df = pd.DataFrame({\n",
    "        \"Date\": dates,  # 格式化當前日期為 YYYY-MM-DD\n",
    "        \"News_Title\": title_list,  # 新聞標題列表\n",
    "        \"source\": sources\n",
    "    })\n",
    "\n",
    "    # 將 DataFrame 儲存為 CSV 文件，文件名稱包含當前日期\n",
    "    csv_filename = f\"News_Title_{datetime.now().strftime('%Y-%m-%d-%H%M%S')}.csv\"\n",
    "    df.to_csv(csv_filename, index=False, encoding='utf-8-sig')  # 不儲存索引，使用 UTF-8 編碼\n",
    "\n",
    "    print(f\"成功儲存新聞標題到 {csv_filename}\")\n",
    "# SaveTitles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNews(domain, keyword, date):\n",
    "    global articles, title_list, dates, sources\n",
    "    \n",
    "    articles = newsapi.get_everything(\n",
    "        q=keyword,\n",
    "        domains=domain,\n",
    "        from_param=date,\n",
    "        to=date,\n",
    "        language='en',\n",
    "        sort_by='publishedAt',\n",
    "        page_size=100\n",
    "    )\n",
    "    \n",
    "    for article in articles['articles']:\n",
    "        title = article['title']\n",
    "        description = article['description']\n",
    "        text = (title or '') + ' ' + (description or '')\n",
    "        dates.append(article['publishedAt'][:10])\n",
    "        title_list.append(text.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AnalyzeSentiment(newsFileName, newNewsFileName):\n",
    "    sentiments = []\n",
    "    newsFile = pd.read_csv(newsFileName)\n",
    "\n",
    "    for i, news in newsFile.iterrows():\n",
    "        title = str(news[\"News_Title\"])\n",
    "        score = sia.polarity_scores(title)['compound']\n",
    "        sentiments.append(sia.polarity_scores(title)['compound'])\n",
    "        print(title)\n",
    "        print(\"sentiment score: \", score)\n",
    "        \n",
    "    pd.concat([newsFile, pd.DataFrame({'Score':sentiments})], axis=1).to_csv(newNewsFileName, header=True, index=False)\n",
    "    print(\"file output to \" + newNewsFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear():\n",
    "    global dates, title_list, sources\n",
    "    dates = []\n",
    "    title_list = []\n",
    "    sources = []\n",
    "clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input some stocks name:  tesla\n"
     ]
    },
    {
     "ename": "NewsAPIException",
     "evalue": "{'status': 'error', 'code': 'rateLimited', 'message': 'You have made too many requests recently. Developer accounts are limited to 100 requests over a 24 hour period (50 requests available every 12 hours). Please upgrade to a paid plan if you need more requests.'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNewsAPIException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m date_list:\n\u001b[0;32m     11\u001b[0m     getNews(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnbc.com\u001b[39m\u001b[38;5;124m'\u001b[39m, keyword, date)\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mgetNews\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myahoo.com\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m SaveTitles()\n",
      "Cell \u001b[1;32mIn[42], line 4\u001b[0m, in \u001b[0;36mgetNews\u001b[1;34m(domain, keyword, date)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNews\u001b[39m(domain, keyword, date):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m articles, title_list, dates, sources\n\u001b[1;32m----> 4\u001b[0m     articles \u001b[38;5;241m=\u001b[39m \u001b[43mnewsapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_everything\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeyword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdomains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_param\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort_by\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpublishedAt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m articles[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticles\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     15\u001b[0m         title \u001b[38;5;241m=\u001b[39m article[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\newsapi\\newsapi_client.py:334\u001b[0m, in \u001b[0;36mNewsApiClient.get_everything\u001b[1;34m(self, q, qintitle, sources, domains, exclude_domains, from_param, to, language, sort_by, page, page_size)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# Check Status of Request\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m requests\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mok:\n\u001b[1;32m--> 334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewsAPIException(r\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[1;31mNewsAPIException\u001b[0m: {'status': 'error', 'code': 'rateLimited', 'message': 'You have made too many requests recently. Developer accounts are limited to 100 requests over a 24 hour period (50 requests available every 12 hours). Please upgrade to a paid plan if you need more requests.'}"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "# Get today's date\n",
    "today = datetime.today()\n",
    "# Start date: 30 days ago\n",
    "start_date = today - timedelta(days=30)\n",
    "# Iterate through dates\n",
    "date_list = [(start_date + timedelta(days=i)).strftime('%Y-%m-%d') for i in range(31)]\n",
    "keyword = input('Input some stocks name: ')\n",
    "# Print the dates\n",
    "for date in date_list:\n",
    "    getNews('cnbc.com', keyword, date)\n",
    "    getNews('yahoo.com', keyword, date)\n",
    "SaveTitles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Collecting requests-oauthlib\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests-oauthlib) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib) (2024.8.30)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: oauthlib, requests-oauthlib\n",
      "Successfully installed oauthlib-3.2.2 requests-oauthlib-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install requests-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "api = twitter.Api(consumer_key='你的consumer_key',\n",
    "   consumer_secret='你的consumer_secret',\n",
    "   access_token_key='你的access_token_key',\n",
    "   access_token_secret='你的access_token_secret')\n",
    "  \n",
    "docs = api.GetSearch(term='台灣', since='2020-01-01', count=25, result_type='popular', return_json=True)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-4.15.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tweepy) (2.32.2)\n",
      "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tweepy) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2024.8.30)\n",
      "Downloading tweepy-4.15.0-py3-none-any.whl (99 kB)\n",
      "   ---------------------------------------- 0.0/99.4 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/99.4 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 20.5/99.4 kB 330.3 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 61.4/99.4 kB 544.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 99.4/99.4 kB 636.7 kB/s eta 0:00:00\n",
      "Installing collected packages: tweepy\n",
      "Successfully installed tweepy-4.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
